# 写写数仓 - 知乎
[写写数仓 - 知乎](https://zhuanlan.zhihu.com/p/468332892) 

### **前言**

干了几年数仓了, 随便写写

### **简单**

### **演变**

首先, 我们从数据仓库的目的或者职责开始说起: 个人认为最大的一个功能就是做报表, 提供报表给管理层, 业务运营做决策; 我干的最多的也是做报表

一般原始是数据存储在关系型数据库中 (mysql 等), 基于原始数据很容易统计一张报表, 写 sql 不行就直接写程序代码; 也就形成了原始数据 -> 报表的架构

在这个架构中有一些问题

-   随着数据量增大, 一个报表统计跑一天
-   有一些计算逻辑, 在很多张报表中重复
-   代码越写越长, 难以维护, 并且如果有底层数据要改, 则涉及到的报表代码全部要改;
-   ...

基于以上, 引入数仓分层的概念, 简单点就是引入中间计算层

原始数据 -> 中间计算层 -> 报表

抽出重复逻辑, 报表只从中间层出数据, 一步计算分为多步, 但同时时效性降低 (t+1);

### **中间计算层如何设计**

目前业界有两种方案, inmon 架构和维度建模架构; 一般都是维度建模; inmon 架构希望建立一个企业层面的自顶向下的符合 3 范式的实体 - 关系模型, 耗时很久; 维度建模是下而上的, 可以快速构建

### **基础技术**

需要实现上面的业务需求, 我们需要了解下基本的基础技术, 单机已经不足以应付目前的数据量, 我们需要寻找新的技术, 大数据一般分为数据存储和数据计算两块;

数据存储

-   数据仓库作为公司的数据中心, 需要集成所有的原始数据, 包括业务数据 (一般在关系型数据库), 埋点日志数据等; 所以我们需要一个存储系统存储所有的数据
-   分布式文件系统 hdfs, 基于 hdfs 的工具有很多, 用作数据仓库的一般是 hive

数据计算

-   数据仓库不生产数据, 只是数据的搬运工; 基于原始数据经过计算得出我们想要的数据
-   分布式计算 mapreduce,spark

数据仓库职责

-   集成数据, 管理数据, 产出结果数据

### **架构**

### **离线**

如果不考虑太多的东西, 仅考虑做报表, 简单搞一个基础架构

![](https://github.com/Hsu-Outer-Brain/WebCliperCDN_001/blob/main/img3/2023-2-17%2017-20-12/f51c3da3-d141-4f76-b931-e6a15c5b52ee.jpeg?raw=true)

解析架构:

### **ods: 用户通过行为产生原始数据, 数仓通过数据抽取工具定时抽取数据到数仓**

```text
1. 规定每一条数据都需要有一个update_time和create_time;用户的每次更新都需要更新update_time
2. 首次抽取全量数据 his
3. 之后的每次抽取都限定update_time,如果是t+1,则限定为昨天 抽取的数据为new;
4. his + new 再根据主键(唯一标识这一条数据)去重并取update_time最新的一条,覆盖his
缺陷
1. 如果数据更新但是没有更新update_time那么就不会更新,数据就会是错误数据;
2. 如果原始数据物理删除了一条数据,ods层还是会有这条数据,不会删除掉;原始数据采用逻辑删除
解决
一般通过数据质量监测,监测出问题后手动处理;比如数据量比对,某一列的计算值比对等
```

通过以上步骤可以在数仓系统还原原始数据, 这一层叫做 ods(贴源层); 这一层的目的就是集成数据, 我们要对数据加工, 首先得有数据, 我们从各个不同的系统抽取数据

### **dwd: 这一层是数据仓库的核心, 暂且不讨论如何设计; 我们有了 ods 的数据, 基于 ods 通过初步计算加工得到 dwd 层, 这个过程我们称为 ETL(加载, 清洗, 转换)**

```text
数据存储:数据进入ods层就已经进入了数仓,数据存储在hive中,hive是一个数据仓库,和数据库一样,有数据库,表,字段等;
数据计算:hive有两部分组成,一个是metasotre,负责hdfs的文件和库表的映射;一个是将hiveSql转换成mapreduce程序;所以hive是可以做为数据计算引擎的,但是spark作为新的计算引擎速度很快,所以一般都会采用spark来做etl
在这里数据是hive_ods通过spark计算再存储在hive_dwd中
```

### **dws: 根据业务需求, 基于 dwd 的各种个性化报表**

### **其他工具**

```text
1.数据通常是t+1更新,也就是今天计算昨天的报表;并且一天只跑一次,通常0点开始,第二天上班前结束;
  1.所以我们需要一个定时任务的调度系统,
  2.同时我们需要依次更新数据,dws依赖dwd的新数据,所以需要dws在dwd更新完之后再启动任务;需要有一个任务依赖功能;
  基于上面的两个功能,开源的工具有azkaban/airflow;
2.跑一个任务不能占用了所以的资源,基于资源隔离,资源限制等需求,选择yarn(hadoop)作为资源调度
3.spark跑一个任务大概的时间在分钟级,用户希望在交互式分析场景中,更快的返回数据,称为即席查询;引入一个纯内存计算引擎presto,资源消耗大,计算快,秒级响应
```

### **后记**

dws 之后根据不同的使用场景需要将数据又抽取出去, 比如用户画像系统, 可以抽取到 hbash 和 es 中, 用户快速检索; 不同场景不同工具就不一一写了;

### **实时架构**

基于变更日志与流计算引擎

![](https://github.com/Hsu-Outer-Brain/WebCliperCDN_001/blob/main/img3/2023-2-17%2017-20-12/260e2d18-ba27-4376-86e0-16fc7b8da2b0.png?raw=true)

架构说明

### **原始数据**

```text
cdc:日志变更捕获,数据的变更日志
技术:canel,debezium,fink-cdc
数据库每次变更数据都会更改日志文件,解析这个日志文件并将变更数据发送至kafka,日志数据本身就是增量数据,直接发往kafka;
```

### **流计算引擎**

```text
flink/spark streaming:流计算引擎,实时处理增量数据
```

### **kv 存储**

```text
kv存储主要用户存储维度表数据,例如:城市维度表,城市id,城市信息;数据库中的表通常只有城市id,需要根据key获取value
```

### **olap**

```text
kudu是一个数据库,数据存储形式类似base+delta的形式,内部维护更新主键,更新时间,以及删除状态;
当查询时,合并base和delta,删除的丢弃,根据主键合并数据取最新数据
```

### **数据链路**

```text
增量数据发到kafka,经过flink消费写入不同存储引擎,如果需求不复杂,可以直接写入olap,但这样会导致烟囱开发,所以实时也会做一些分层;但不宜过多,每一层都会带来延迟;所以这里第一次轻度计算后又发往kafka,再计算存入kudu
流计算只能处理增量数据,统计指标也只能基于his+new的形式,例如,sum值,sum_his+sum_new;但如果是discount便会比较麻烦,不过也有算法可以解决;
流计算有计算误差但是时效性高,离线计算稳定但是时效低,目前大部分采用lamda架构,定时使用离线计算的结果覆盖实时计算
类似上图的架构称为kappa架构
```

### **数据湖, 流批一体**

![](https://github.com/Hsu-Outer-Brain/WebCliperCDN_001/blob/main/img3/2023-2-17%2017-20-12/f7513938-b380-4dc5-b599-da70e0d18917.png?raw=true)

### **能力**

```text
flink:需要提供流处理和批处理计算能力,目前,流计算flink更好,而批处理spark更好,我们更希望合二为一;
hudi:是一个介于数据存储和数据计算中间层级的工具,理解为文件组织形式,base+delta;类似工具有iceberg等
  1. 提供olap查询能力,kafka不能提供查询; 
  2. 支持读取增量更新数据
```

### **数据链路**

```text
日志数据进入ods,提供增量数据给流处理,并提供正确数据给olap查询(日志数据都是同一主键有多条数据,当查询时,需要合并),批处理也是用的查询;
```

### **后记**

简单的架构就是上面这些, 如果要自己搭建一套的话, mysql+zookeeper+hadoop+hive+azkaban+presto+superset+sqoop, 纯开源, 其他组件根据需求更改, cdh 全家桶欢迎你; 有了底层工具, 接下来就开始讲讲数据仓库是怎么设计的, ods,dwd,dws;

### **数据仓库**

### **维度建模**

一般会采用维度建模的方法论来构建数仓, 先来了解几个概念

```text
关系型数据库中的数据我们一般称为操作型数据,主要用于oltp(事务),而数仓中的数据主要面向决策,数据决策型数据,用于olap(分析);
olap中的核心概念是cube:olap多维分析
案例:假设付款场景中,一笔付款包括字段
  日期,用户姓名,付款地点省份,付款方式,付款状态(失败/成功),付款金额
对于以上数据,我们统计某天某个省份的某种付款方式成功的付款金额;我需要对前面的维度条件随意选择,随意限制;这叫做维度的上卷,下钻,切面,切块;
由上面的数据知道,为了统计数据,我们需要两种数据:维度(筛选的条件) 和 度量(统计的数值);通常这样的表我们叫做事实表
对上面的用户姓名字段,通常不会定义成姓名,而是用户id,因为用户有很多属性,包括性别,年龄,收入等,付款地点省份通常也会换成门店id等;那么用户id和外部用户维度表关联,门店id和外部门店维度表关联;这样,一张事实表和多张维度表的关联像一个星星,叫做星型模型;
维度建模就是构造很多这样的星型模型
```

### **星型模型**

-   事实

组成: 维度 + 度量

如何构造星型模型? 星型模型最核心的就是中间的事实表, 那事实表是怎么来的呢?

首先, 有一个业务, 这个业务可以化为多个业务过程, 每个业务过程都是用户来操作的, 用户的每一步操作都需要一条数据来度量, 那这条数据就是事实表的一条数据;

这个业务我们叫做数据域, 业务过程就叫业务过程, 每一个业务过程都有对应的事实表;

-   维度

组成: 维度 id + 维度属性

观察事物的角度, 在上面的例子中, 日期是一个维度, 用户是一个维度... 我们会将维度提取出来, 在事实表中只留下一个 id, 当然为了减少关联或者是懒得做维度表, 也会在事实表中冗余维度, 例如上面的字段, 会将用户姓名和用户 id 都保留; 付款状态不会做成维度表, 这种维度称为退化维度;

### **汇总**

组成: 统计粒度 + 统计指标

有了星型模型, 统计数据就很快了, 选择统计粒度 (维度), 对度量做统计 (统计指标), 就完事了;

```text
指标管理:指标多了之后,不知道指标的统计口径是啥,两个相似命名的统计指标不知道哪个是对的;每个人的命名风格不一样等;
针对这些问题,需要建设一个指标管理平台来管理指标,统一命名等;
```

### **总线矩阵**

事实根据数据域 + 业务过程划分, 但是维度呢, 有些维度是跨数据域的, 例如用户维度; 我们需要建设一个一致性维度, 所以设计一个总线矩阵, 行是业务过程, 列是维度, 每一格表示业务过程和维度是否有关联关系

### **其他**

-   分层: 分层几乎差不多就是 3 层, 根据情况自己再分; 这里还有一个数据集市层没写, 数据集市可以理解为部门数据仓库; 针对一个部门建设的数据仓库, 通常这一层由部门自己维护
-   命名规范: 每个层级的库表命名与统计指标命名, 每一个需要命名的地方都需要有规范;

![](https://github.com/Hsu-Outer-Brain/WebCliperCDN_001/blob/main/img3/2023-2-17%2017-20-12/ae0277f7-be65-4ad1-8609-48826a167676.jpeg?raw=true)

以上是简简单单, 粗粗糙糙的写了个维度建模的大概, 细节就不多说了;

### **数据仓库平台**

基于上面的架构和维度建模, 一个简单的数仓就出来了; 早期确实是这样的, 随着发展, 大家都开始做平台了, 屏蔽底层, 不用再黑窗口敲命令;

做平台就需要将之前要做的事全部搬到平台上来, 可以将功能划分几个块

-   数据集成: 将外部数据源集成到数据仓库
-   数据查询: 即席查询, 开源: hue
-   数据管理: 根据维度建模设计表, 字段;
-   任务调度: 任务依赖
-   数据可视化: olap 多维分析, 开源: superset
-   权限管控:
-   数据质量:
-   数据源管理

### **数据集成**

数据集成就是收集所有的外部数据到数仓中来; 外部数据一般是日志数据与其他数据库数据; 还有一种临时上传数据, 这种给个接口上传, 再映射成临时表就可以了, 一般在数据查询功能中做, 这里就不考虑了;

数据集成有离线和实时两部分

### **离线**

```text
离线之前已经写过了,首次抽取全量数据,每次更新根据update_time抽取更新数据(通常是抽取>=昨天,考虑延迟数据,任务00:15跑),再根据拉链表的方式还原最新数据;如果源数据是不会修改的数据(例如流水),那么就不用拉链表,直接抽取最新数据合并就可以了;
之前也讲过,这种抽取方式会有两个问题:删除的数据数仓感知不到并一直存在,修改数据缺不修改update_time感知不到;
删除数据:通过增加create_time,并定时对昨日以及之前的创建数据量对比,可以感知到
修改数据:这个就难搞了,粗糙点根据某些列的统计值做判断,精细点就根据create_time抽取昨日以及之前的数据做成文件,两边对比;算法:crc,md5等
这两种情况属于数据质量建设了
```

### **实时**

```text
埋点日志数据:日志数据一般是业务团队发到kafka,由数仓使用flink消费kafka写入数仓,这种数据是只增数据,hive/hbase都可以;
数据库日志数据:例如mysql,使用canel/debezium 可以捕获数据变更日志;将日志发送到kafka,然后由flink消费写入kudu;由于是数据变更日志,(一条数据发生变更就会有两条日志数据,同一主键),需要一个支持行级别更新的数据存储引擎,kudu/hudi等都可以;
kudu/hudi:他们的数据模式是base+delta的模式,查询的时候会合并,底层也会定时合并,去掉冗余数据(包括标记为删除的数据)
```

### **平台功能**

```text
1. 一键同步:选择源表,对应生成ods表,调度;实时的也会对应生成kafka的topic;启动全量同步任务与增量同步任务;
2. 重建同步:当源表schema发生变动时,需要重新生成同步任务;
3. 分库分表:
  1. ods层之前还隐藏了一层预处理层,从源表抽取数据后不会直接写入ods,先存放到预处理层再处理,分库分表可以在预处理层还是一对一,在ods层将分库分表合并;
  2. 上面的有个小问题吧,100张表就有100个任务;如果分库分表是在一个数据库实例中,且表名是有规则的,可以全部拼接为一条sql一次性抽取;对应预处理层也是一张表
4. 监控:数据同步监控数据,属于数据质量范畴,但是要在这里给一个可视化展示
```

### **数据查询**

简单点就是即席查询, 运行 sql, 返回结果; 开源平台 hue 就很不错

```text
1. 即席查询(不同引擎)
2. 提供临时导入导出数据的功能(临时表)
3. 权限控制:查询数据的时候必然会出现权限的问题,这里给个权限申请入口
```

### **数据管理**

这个平台是数仓开发的核心平台, 通常是集成开发环境平台, 在这个平台需要将数仓开发的表全部管理起来;

```text
1. 规划业务:数据域,业务过程,事实,维度,汇总,指标等;
2. 脚本管理:etl开发就是写sql,sql就是业务逻辑,一个sql脚本的作用就是从源数据经过计算生成新表数据;
3. 指标管理:指标管理直接关系到数据使用,指标过多就会导致统计口径,重复建设问题;
```

### **任务调度**

一个 sql 脚本就是一个任务, 必需等待源表任务跑完才能启动任务;

功能就是: 定时任务与任务依赖, 开源方案: azkaban/airflow

### **数据可视化**

数仓的主要任务之一是报表, 数据可视化就是将报表展示在页面上, 并且在辅以其他功能

```text
1. 展示报表:报表需要秒级返回
2. 多维分析(olap):维度筛选,条件筛选
3. 图表分析:可以做图
4. 邮件发送:做一个看板,然后定时发送邮件
概念
olap:简单点就是 where 条件 group by 维度 + 数值统计;维度是随便几个维度,条件也是随机的,如何让这个sql计算的快
  rolap: 即席查询,表设计成星型模型一样,事实表关联维度表计算;这样需要强大的计算能力;
    一般会选择presto(秒级返回),但是presto消耗资源很多,任务过多或过大会导致奔溃,替代工具如clickhouse等olap引擎;
  molap: 预计算查询;预先计算全维度cube的数据,查询时,将计算任务直接转变成查询;类似工具kylin,druid
优化
报表要的是数据返回够快,等一分钟人就不想看了,架构方面,redis缓存+kylin定制查询+presto即席查询;
5. 实时大屏:针对实时任务做单一指标的统计看板.
```

### **权限管控**

大数据涉及的平台很多, 需要一个统一的权限管控平台, 并将整个平台的权限串起来

```text
库表的权限与各个平台的权限
1. 防君子不防小人:更多的是防止误操作
2. 权限申请审批人由各业务线自己审批
```

### **数据源管理**

每个平台都要数据源链接信息, 如果每个平台自己搞一套估计用户用起来就奔溃了吧

### **数据质量**

搞一套数仓出报表还是很快的, 架构一搭, sql 一跑, 报表就出来了; 报表数据准不准呢? 那就懵逼了; 跑上一段时间, 各个环节就会出现一些问题了. 这个时候数据质量就是重中之重;

### **衡量**

数据完整性, 一致性, 准确性, 及时性;

### **方法**

-   卡点校验

```text
1.从数据接入开始
 监控源表与ods的数据一致,schema变更,以及抽取数据的数据量(一天更新上亿,原有资源会跑的太慢,导致整体任务等待)
2.对表的数据质量监控
 每次etl更新完数据后,需要根据质量规则对表数据打分:包括不限于:数据量,主键唯一数量,空值,最大最小值,与源表的统计值比较,枚举值一场等
3.sql脚本检验
 语法检测
4.对任务时间监控
 任务耗时过久,任务异常告警,以及业务数据异常
```

-   资产评级

```text
对表资产打标签评级;
重要任务优先保障,特别是在任务资源不足时
```

### **其他**

-   元数据

```text
元数据分为技术元数据和业务元数据
业务元数据:在数据管理平台中对表打上的很多业务的标签,例如数据域,业务过程等
技术元数据:表的字段,分区,任务调度,sql脚本等
这些数据也可以提取出来作为一张表的质量信息展示
```

-   数据血缘

```text
etl:源表数据->计算->目标表数据;sql脚本承载
我们需要一个清晰的数据流向血缘图;
血缘有表级血缘和字段级血缘,当然用图数据库存储是很好的;
1. 表级血缘:hive本身有个钩子,但是好像我们etl都用spark了,使用spark explain也能解析;我们还是使用antlr4然后找到spark源码的g4文件自己解析sql脚本吧;
2. 字段级血缘:也是用antlr4;
3. 跨存储血缘:上面这个在hive中的血缘,如果要做跨存储的,比如源表和ods的;
```

好了, 以上就简单写了下数仓的平台

### **指标体系建设**

报表, 简单点就是统计指标, 如何设计指标呢? 很多时候都是业务给需求, 我们统计, 自己也可以想想指标;

### **方法论**

aarrr 模型 + osm+mece + 分层指标

### **具体操作**

### **解释**

```text
aarrr:拉新,活跃,留存,变现,推荐
osm:目标,策略,度量
mece:相互独立,完全穷尽
分层:一般指标分为一级指标,二级指标,三级指标
```

### **操作**

```text
一个业务,基本都是按照aarrr模型来做运行的;
明显aarrr的五个部分就是osm的目标,例如拉新,拉新是我们的目标,每个部门针对拉新肯定会有很多业务营销(策略),那么在业务营销中会产生很多的度量,基于此,一级指标就容易得出来了;我们用一级指标来简单衡量这个业务营销的好坏,通常一级指标都比较宽泛
基于一级指标,我们还可以下探很多细致的指标,那这个如何做呢,那么我们就需要将这个指标拆分,基于mece法则一直拆下去;
案例:
拉新
一级指标:注册人数
二级指标:pv,转化率
三级指标:进入页面时长,进入页面的渠道,点击注册按钮人数等
```

### **用户画像**

现在每个公司都要做用户画像吧, 简单点就是对用户打标签, 标签有两种: 一种是简单计算就能得出的, 还有一种是通过机器学习训练出来的

### **etl**

```text
一般就是根据etl来的,首先根据业务(数据域)统计各个业务的子标签表,然后汇聚所有子标签表做成总标签表
```

### **存储**

```text
这个有两种存储的方式,
一种是横表,schema:user_id,标签1,标签2...这种就是用起来方便,直接写sql查就行;但是呢,标签通常上千个,宽表真宽
还有一个是竖表,schema:user_id,标签id,标签值;这种呢,没那么多列,但是行数暴涨,user_cnt * 标签_cnt;这种对废弃标签比较友好
```

### **使用**

```text
一般还是用横表;
用户画像要的就是根据条件快速圈选人群,毫秒级;这一点全文检索很合适;
通常会在hive中统计好所有的标签值,再导入到es中,前端根据条件查询es;
es中根据筛选条件快速筛选出user_id,如果数据量实在太大,可以将数据再次导入到hbase中,es筛选user_id,再根据user_id
在hbase查询所有数据
ps:es中存储的数据是倒排索引,也就是根据value查询key
hbase存储数据的正排索引,根据key查询value
hbase根据user_id查询标签值是很快的,但是es就稍差
```
